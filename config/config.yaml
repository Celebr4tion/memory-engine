# Base Configuration for Memory Engine
# This file contains default configuration values that apply to all environments

# Application Settings
environment: development
debug: false

# Database Configuration
database:
  url: "sqlite:///memory_engine.db"
  pool_size: 10
  max_overflow: 20
  pool_timeout: 30
  pool_recycle: 3600

# Storage Configuration
storage:
  graph:
    backend: "janusgraph"  # Options: "janusgraph", "json_file", "sqlite"
    
    # JanusGraph Configuration
    janusgraph:
      host: "localhost"
      port: 8182
      use_ssl: false
      connection_timeout: 30
      max_retry_attempts: 3
      retry_delay: 1.0
    
    # JSON File Backend Configuration
    json_file:
      directory: "./data/graph"
      pretty_print: true
    
    # SQLite Backend Configuration
    sqlite:
      database_path: "./data/knowledge.db"

# Legacy JanusGraph Configuration (for backwards compatibility)
janusgraph:
  host: "localhost"
  port: 8182
  use_ssl: false
  connection_timeout: 30
  max_retry_attempts: 3
  retry_delay: 1.0

# Modular Embedding System Configuration
embeddings:
  # Provider Configuration
  provider: "gemini"  # Options: "gemini", "openai", "sentence_transformers", "ollama"
  
  # Provider-specific configurations
  gemini:
    api_key: null  # Set via GOOGLE_API_KEY environment variable
    model_name: "gemini-embedding-exp-03-07"
    dimension: 768
    max_batch_size: 32
    timeout: 30
  
  openai:
    api_key: null  # Set via OPENAI_API_KEY environment variable
    model_name: "text-embedding-3-small"
    dimension: 1536
    max_batch_size: 100
    timeout: 30
    organization: null  # Optional
    base_url: null  # Optional custom endpoint
  
  sentence_transformers:
    model_name: "all-MiniLM-L6-v2"
    device: "cpu"  # Options: "cpu", "cuda", "auto"
    max_batch_size: 64
    trust_remote_code: false
    normalize_embeddings: true
    cache_folder: null  # Optional custom cache location
  
  ollama:
    model_name: "nomic-embed-text"
    base_url: "http://localhost:11434"
    max_batch_size: 32
    timeout: 60
    keep_alive: "5m"

# Vector Store Configuration
vector_store:
  # Backend Configuration
  backend: "numpy"  # Options: "milvus", "chroma", "numpy", "qdrant", "faiss"
  
  # Backend-specific configurations
  milvus:
    host: "localhost"
    port: 19530
    collection_name: "knowledge_vectors"
    dimension: 768
    metric_type: "L2"
    index_type: "IVF_FLAT"
    nlist: 1024
    nprobe: 10
    user: null  # Optional authentication
    password: null  # Optional authentication
  
  chroma:
    path: null  # None for in-memory, path for persistent storage
    collection_name: "knowledge_vectors"
    dimension: 768
    metric_type: "L2"  # Options: "L2", "COSINE", "IP"
    batch_size: 1000
    # Optional server mode
    host: null
    port: null
    ssl: false
    headers: {}
  
  numpy:
    collection_name: "knowledge_vectors"
    dimension: 768
    metric_type: "L2"  # Options: "L2", "COSINE", "IP"
    persist_path: "./data/numpy_vectors"
    auto_save: true
    max_memory_usage: 1000  # MB
  
  qdrant:
    host: "localhost"
    port: 6333
    collection_name: "knowledge_vectors"
    dimension: 768
    metric_type: "COSINE"
    api_key: null  # Optional authentication
    https: false
    timeout: 60
  
  faiss:
    collection_name: "knowledge_vectors"
    dimension: 768
    metric_type: "L2"
    index_type: "IndexFlatL2"
    persist_path: "./data/faiss_vectors"
    nlist: 1024  # For IVF indexes
    use_gpu: false

# Legacy Embedding Configuration (for backwards compatibility)
embedding:
  model: "gemini-embedding-exp-03-07"
  dimension: 768
  batch_size: 32
  max_retries: 3
  retry_delay: 1.0
  timeout: 30

# LLM Configuration
llm:
  # Default provider
  provider: "gemini"  # Options: "gemini", "openai", "ollama", "anthropic", "huggingface"
  
  # Legacy configuration (for backwards compatibility)
  model: "gemini-2.0-flash-thinking-exp"
  fallback_model: "gemini-2.0-flash-exp"
  temperature: 0.7
  max_tokens: 4096
  timeout: 60
  max_retries: 3
  retry_delay: 1.0
  
  # Provider-specific configurations
  gemini:
    api_key: null  # Set via GOOGLE_API_KEY environment variable
    model_name: "gemini-2.0-flash-thinking-exp"
    fallback_model: "gemini-2.0-flash-exp"
    temperature: 0.7
    max_tokens: 4096
    timeout: 60
    safety_settings: null
    generation_config: null
  
  openai:
    api_key: null  # Set via OPENAI_API_KEY environment variable
    model_name: "gpt-4o-mini"
    temperature: 0.7
    max_tokens: 4096
    timeout: 30
    organization: null
    base_url: null
    top_p: 1.0
    frequency_penalty: 0.0
    presence_penalty: 0.0
  
  ollama:
    base_url: "http://localhost:11434"
    model_name: "llama2"
    temperature: 0.7
    max_tokens: 4096
    timeout: 60
    top_p: 0.9
    top_k: 40
    keep_alive: "5m"
    repeat_penalty: 1.1
  
  anthropic:
    api_key: null  # Set via ANTHROPIC_API_KEY environment variable
    model_name: "claude-3-5-sonnet-20241022"
    temperature: 0.7
    max_tokens: 4096
    timeout: 30
    top_p: 0.9
    top_k: null
  
  huggingface:
    api_key: null  # Set via HUGGINGFACE_API_KEY environment variable
    model_name: "microsoft/DialoGPT-medium"
    temperature: 0.7
    max_tokens: 4096
    timeout: 30
    device: "cpu"
    torch_dtype: "auto"

# API Configuration (API keys loaded from environment variables)
api:
  gemini_api_key: null  # Set via GEMINI_API_KEY environment variable
  google_api_key: null  # Set via GOOGLE_API_KEY environment variable

# Logging Configuration
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file_path: null
  max_file_size: 10485760  # 10MB
  backup_count: 5
  enable_console: true

# Versioning Configuration
versioning:
  enable_versioning: true
  enable_snapshots: true
  changes_threshold: 100
  snapshot_interval: 3600
  max_revisions: 1000
  compression_enabled: true

# Security Configuration
security:
  encrypt_at_rest: false
  encryption_key: null  # Set via ENCRYPTION_KEY environment variable
  tls_enabled: false
  max_login_attempts: 5
  session_timeout: 3600

# Performance Configuration
performance:
  workers: 4
  worker_timeout: 300
  max_memory_usage: 1073741824  # 1GB
  cache_size: 1000
  batch_processing_size: 100